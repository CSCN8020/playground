{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Code is mostly re-used from https://github.com/moduIo/Deep-Q-network/blob/master/DQN.ipynb\n",
    "# Code may change with more clean-ups and explanation\n",
    "import gym\n",
    "from collections import deque\n",
    "!pip install gym[atari]\n",
    "# GOOGLE: install gym atari accept-rom-license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dqn_agent import DQN_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import blend_images, process_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4', render_mode=\"rgb_array\")\n",
    "state_size = (105, 80, 1)\n",
    "action_size = env.action_space.n\n",
    "minimum_memory_size = 1000 # Minimum experience replay memory size before training\n",
    "max_memory_size = 10000 # Maximum experience replay memory\n",
    "agent = DQN_Agent(state_size, action_size, memory_size=max_memory_size, epsilon_decay=0.99)\n",
    "\n",
    "episodes = 100\n",
    "batch_size = 64\n",
    "skip_start = 90  # Breakout-v0 waits for 90 actions before the episode begins\n",
    "total_time = 0   # Counter for total number of steps taken\n",
    "all_rewards = 0  # Used to compute avg reward over time\n",
    "blend = 4        # Number of images to blend\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tensorboard logs (plots)\n",
    "import datetime\n",
    "import tensorflow.summary as summary_writer\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "train_log_dir = './logs/decay99_' + current_time + '/train'\n",
    "train_summary_writer = summary_writer.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize processed frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Visualize state\n",
    "observation = env.reset()\n",
    "observation = env.step(1)\n",
    "for skip in range(2): # skip the start of each game/\n",
    "    observation = env.step(0)\n",
    "\n",
    "# observation = observation[100::]\n",
    "processed_observation = process_frame(observation[0])\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_state(state, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(state, cmap='gray')\n",
    "    # plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "# show_state(env.render())\n",
    "\n",
    "show_state(processed_observation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Steps\n",
    "1. Set up environment\n",
    "(Assuming image as a state)\n",
    "2. Visualize the state/observation\n",
    "3. Pre-processing and observe the post-processed state\n",
    "4. Create a random agent and visualize output for a couple of episodes\n",
    "5. Set up the logging (tensorboard recommended). Choose the metrics, create the training loop and save the metrics\n",
    "6. Save and load model\n",
    "7. Run training loop (without actual algorithm) and make sure all metrics are saved correctly + model is saved\n",
    "8. Stop training and make sure you can load and continue training from saved checkpoint\n",
    "9. Implement the algorithm and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blend = 4\n",
    "max_steps_per_episode = 5000\n",
    "for e in range(episodes):\n",
    "    total_reward = 0\n",
    "    game_score = 0\n",
    "    observation = env.reset()\n",
    "    # print(observation)\n",
    "    state = process_frame(observation[0])\n",
    "    images = [state]*blend\n",
    "    \n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        env.step(0)\n",
    "    \n",
    "    # generate an episode\n",
    "    for time in range(max_steps_per_episode):\n",
    "        total_time += 1\n",
    "        \n",
    "        # Every update_rate timesteps we update the target network parameters\n",
    "        if total_time % agent.update_rate == 0:\n",
    "            # Update the target model by copying weights from Qnetwork\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Return the avg of the last 4 frames\n",
    "        state = blend_images(images, state_size, blend)\n",
    "        \n",
    "        # Choose and apply action\n",
    "        action = agent.act(state)\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # TODO: Process the frame and save it to memory\n",
    "        # 1. pre-process the image (grayscale conversion, crop, resize...)\n",
    "        processed_state = process_frame(next_observation)\n",
    "        # 2. Combine with previous images in the image circular buffer (size=4)\n",
    "        images.pop(0)\n",
    "        images.append(processed_state)\n",
    "        next_state = blend_images(images, state_size, blend)\n",
    "        # 3. Add to the replay memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update state, and rewards\n",
    "        state = next_state\n",
    "        game_score += reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            all_rewards += game_score\n",
    "            \n",
    "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
    "                  .format(e+1, episodes, game_score, total_reward, all_rewards/(e+1), time, total_time))   \n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > minimum_memory_size and (total_time % 500) == 0:\n",
    "            # train\n",
    "            agent.replay(batch_size)\n",
    "    agent.update_exploration_rate()\n",
    "    with train_summary_writer.as_default():       \n",
    "        summary_writer.scalar('game_score', game_score, step=e)\n",
    "    # TODO: Save model every n (try 10 or 25) episodes\n",
    "    if e % 10 == 0:\n",
    "        fname = f'models/{max_memory_size}-memory_{e}-games'\n",
    "        agent.save(fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# agent.save('models/5k-memory_100-games')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "!pip install IPython\n",
    "!pip install matplotlib\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render())\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "game_score = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "reward = 0\n",
    "env.reset()\n",
    "images = deque(maxlen=blend)\n",
    "for t in range(2000):\n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        env.step(0)\n",
    "    show_state(env, t)\n",
    "    total_time += 1\n",
    "    \n",
    "    # Return the avg of the last 4 frames\n",
    "    state = blend_images(images, state_size, blend)\n",
    "    \n",
    "    # Transition Dynamics\n",
    "    action = agent.greedy_act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    # Return the avg of the last 4 frames\n",
    "    next_state = process_frame(next_state)\n",
    "    images.append(next_state)\n",
    "    next_state = blend_images(images, state_size, blend)\n",
    "        \n",
    "    state = next_state\n",
    "    game_score += reward\n",
    "    reward -= 1  # Punish behavior which does not accumulate reward\n",
    "    total_reward += reward\n",
    "    time.sleep(0.05)\n",
    "    if done:\n",
    "        all_rewards += game_score\n",
    "        \n",
    "        print(\"game score: {}, reward: {}\"\n",
    "                .format(game_score, total_reward))\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cscn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
